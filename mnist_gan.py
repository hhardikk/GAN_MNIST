# -*- coding: utf-8 -*-
"""MNIST_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TeUq2BbBG3SPcAtzU69QuaJ_DVsgEsn9
"""

from numpy import expand_dims
from numpy import zeros
from numpy import ones
from numpy import vstack
from numpy.random import randn
from numpy.random import randint
from keras.datasets.mnist import load_data
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Reshape
from keras.layers import Flatten
from keras.layers import Conv2D
from keras.layers import Conv2DTranspose
from keras.layers import LeakyReLU
from keras.layers import Dropout
from matplotlib import pyplot as plt

def discriminator(in_shape = (28, 28, 1)):
  model = Sequential()
  model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same', input_shape = in_shape))
  model.add(LeakyReLU(alpha = 0.2))
  model.add(Dropout(0, 4))
  model.add(Conv2D(64, (3,3), strides = (2,2), padding = 'same'))
  model.add(LeakyReLU(alpha = 0.2))
  model.add(Dropout(0, 4))
  model.add(Flatten())
  model.add(Dense(1, activation = 'sigmoid'))

  optimizer = Adam(lr = 0.0002, beta_1 = 0.5)
  model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics=['accuracy'])
  return model

def generator(latent_dim):
  model = Sequential()
  n_nodes = 128*7*7
  model.add(Dense(n_nodes, input_dim = latent_dim))
  model.add(LeakyReLU(alpha = 0.2))
  model.add(Reshape((7, 7, 128)))

  #Upsample to 14x14
  model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))
  model.add(LeakyReLU(alpha = 0.2))

  #Upsample to 28x28
  model.add(Conv2DTranspose(128, (4,4), strides = (2,2), padding = 'same'))
  model.add(LeakyReLU(alpha = 0.2))

  model.add(Conv2D(1, (7,7), activation = 'sigmoid', padding = 'same'))
  return model

def gan(g_model, d_model):
  d_model.trainable = False
  model = Sequential()
  model.add(g_model)
  model.add(d_model)
  opt = Adam(lr = 0.0002, beta_1 = 0.5)
  model.compile(loss = 'binary_crossentropy', optimizer = opt)
  return model

def load_real_samples():
  (trainX, _), (_, _) = load_data()
  X = expand_dims(trainX, axis = -1)
  X = X.astype('float32')
  X = X/255.0
  return X

def generate_real_samples(dataset, n_samples):
  ix = randint(0, dataset.shape[0], n_samples)
  X = dataset[ix]
  Y = ones((n_samples, 1))
  return X, Y

def generate_latent_points(latent_dim, n_samples):
  x_input = randn(latent_dim, n_samples)
  x_input = x_input.reshape(n_samples, latent_dim)
  return x_input

def generate_fake_samples(g_model, latent_dim, n_samples):
  x_input = generate_latent_points(latent_dim, n_samples)
  X = g_model.predict(x_input)
  Y = zeros((n_samples, 1))
  return X, Y

def save_plot(examples, epoch, n = 10):
  for i in range(n**2):
    plt.subplot(n, n, i+1)
    plt.axis('off')
    plt.imshow(examples[i, :, :, 0], cmap = 'gray_r')
    filename = 'generated_plot'
    plt.savefig(filename)
    plt.close()

def summarise_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples = 100):
  x_real, y_real = generate_real_samples(dataset, n_samples)
  _, acc_real = d_model.evaluate(x_real, y_real, verbose = 0)

  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)
  _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose = 0)

  print(f"Accuracy Real - {(acc_real * 100)}, Accuracy Fake - {(acc_fake * 100)}")

  save_plot(x_fake, epoch)

  filename = 'generated_model' 
  g_model.save(filename)

def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs = 5, n_batch = 256):
  batch_per_epoch = int(dataset.shape[0]/ n_batch)
  half_batch = int(n_batch / 2)
  for i in range(n_epochs):
    for j in range(batch_per_epoch):
      x_real, y_real = generate_real_samples(dataset, half_batch)
      x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)
      x, y = vstack((x_real, x_fake)), vstack((y_real, y_fake))
      d_loss, _ = d_model.train_on_batch(x, y)

      x_gan = generate_latent_points(latent_dim, n_batch)
      y_gan = ones((n_batch, 1))
      g_loss = gan_model.train_on_batch(x_gan, y_gan)

      print(">%d, %d/%d, d=%.3f, g=%.3f" % (i+1, j+1, batch_per_epoch, d_loss, g_loss))
    summarise_performance(i, g_model, d_model, dataset, latent_dim)

latent_dim = 100
d_model = discriminator()
g_model = generator(latent_dim)
gan_model = gan(g_model, d_model)
dataset = load_real_samples()

train(g_model, d_model, gan_model, dataset, latent_dim)

def save_plot(examples, n):
  for i in range(n * n):
    plt.subplot(n, n, 1 + i)
    plt.axis("off")
    plt.imshow(examples[i, :, :, 0], cmap="gray_r")
  plt.show()
  filename = 'generated_plot'
  plt.savefig(filename)

latent_points = generate_latent_points(100, 25) 
X = g_model.predict(latent_points)
# plot the result
save_plot(X, 5)

from numpy import asarray
vector = asarray([[0.5 for _ in range(100)]])

X = g_model.predict(vector)
plt.imshow(X[0, :, :, 0], cmap = 'gray_r') 
plt.show()
filename = 'generated_plot'
plt.savefig(filename)